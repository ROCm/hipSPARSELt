---
include: hipsparselt_common.yaml
include: known_bugs.yaml
include: spmm_common.yaml

Definitions:
  - &alpha_beta_range
    - { alpha:  5, beta:  0 }
    - { alpha:  0, beta:  3 }
    - { alpha:  1, beta:  3 }
    - { alpha:  1, beta:  1 }

  - &alpha_beta_range_small
    - { alpha: 2, alphai: 2, beta: -1.0, betai: 2.0 }

  - &transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }
    - { transA: T, transB: T }

  - &deepbench_alpha_beta_range
    - { alpha: 1, beta: 0 }
    - { alpha: 1, beta: 1 }

  - &deepbench_transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }

  - &ldd_transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }
    - { transA: T, transB: T }


Tests:
- name: spmm_bad_arg
  category: pre_checkin
  function:
    - spmm_bad_arg: *real_precisions_2b
  transA: N
  transB: N
  fortran: [ false, true ]

# Tests confirm no NaN propagation when alpha = 0, 2 and beta = 0. Value .NaN is converted into zero
- {name: alpha_beta_zero_NaN, category: pre_checkin, precision: *real_precisions_2b,
   function: spmm, transA: N, transB: N, M: 256, N: 128, K:  64, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }


# Split *real_precisions_2b into *int8 and *nonint8_real_precisions. Since int8 has flags 0,1

- name: spmm_small
  category: quick
  function:
    spmm: *real_precisions_2b
  matrix_size: *small_matrix_size_range
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range
  bias_vector: [false, true]
  bias_stride: [0, -1, 256]
  bias_type: [f32_r, f16_r]

- name: spmm_medium
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  matrix_size: *medium_matrix_size_range
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range

- name: spmm_medium_alt
  category: pre_checkin
  function:
    spmm: *hpa_half_precision
  matrix_size: *medium_matrix_size_range
  transA_transB: *transA_transB_range
  alpha: 1
  beta: 0
  initialization: special

- name: spmm_medium_HMM
  category: HMM
  function:
    spmm: *real_precisions_2b
  matrix_size: *medium_matrix_size_range
  transA: [ N ]
  transB: [ N ]
  alpha: 1
  beta: 1
  HMM: true

- name: spmm_chunk
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  matrix_size: *chunk_matrix_size_range
  transA_transB: *transA_transB_range
  alpha: 2
  beta: 3

# Split *int8_half_single_precisions into *int8 and *half_single_precisions. Since int8 has flags 0,1

- name: spmm_deepbench
  category: nightly
  function:
    spmm: *real_precisions_2b
  matrix_size: *deepbench_sizes
  alpha_beta: *deepbench_alpha_beta_range
  transA_transB: *deepbench_transA_transB_range

- name: spmm_deepbench_alt
  category: nightly
  function:
    spmm: *hpa_half_precision
  matrix_size: *deepbench_sizes
  alpha: 1
  beta: 0
  flags: 4
  initialization: special
  transA_transB: *deepbench_transA_transB_range

- name: resnet50_fwd
  category: nightly
  function:
    spmm: *real_precisions_2b
  transA: N
  transB: N
  matrix_size: *resnet50_fwd_sizes
  alpha: 1
  beta: 0

- name: resnet50_fwd_alt
  category: nightly
  function:
    spmm: *hpa_half_precision
  transA: N
  transB: N
  matrix_size: *resnet50_fwd_sizes
  alpha: 1
  beta: 0
  initialization: special

- name: resnet50_bwdwrw
  category: nightly
  function:
    spmm: *real_precisions_2b
  transA: T
  transB: N
  matrix_size: *resnet50_bwdwrw_sizes
  alpha: 1
  beta: 1

- name: resnet50_bwddata
  category: nightly
  function:
    spmm: *real_precisions_2b
  transA: N
  transB: T
  matrix_size: *resnet50_bwddata_sizes
  alpha: 1
  beta: 0

- name: inception4_fwd
  category: nightly
  function:
    spmm: *real_precisions_2b
  transA: N
  transB: N
  matrix_size: *inception4_fwd_sizes
  alpha: 1
  beta: 0

- name: inception4_bwdwrw
  category: nightly
  function:
    spmm: *real_precisions_2b
  transA: T
  transB: N
  matrix_size: *inception4_bwdwrw_sizes
  alpha: 1
  beta: 1

- name: inception4_bwddata
  category: nightly
  function:
    spmm: *real_precisions_2b
  transA: N
  transB: T
  matrix_size: *inception4_bwddata_sizes
  alpha: 1
  beta: 0

- name: ctest_bwdwrw
  category: nightly
  function:
    spmm: *real_precisions_2b
  transA: T
  transB: N
  alpha: 1
  beta: 1
  matrix_size: *ctest_bwdwrw_sizes

- name: ctest_fwd
  category: nightly
  function:
    spmm: *real_precisions_2b
  transA: N
  transB: N
  alpha: 1
  beta: 0
  matrix_size: *ctest_fwd_sizes

- name: spmm_8
  category: quick
  function:
    spmm: *real_precisions_2b
  M: 8
  N: 8
  K: 8
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_16
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 16
  K: 16
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_24
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 24
  N: 24
  K: 24
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_algo
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 32
  N: 32
  K: 32
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  spmm_algo: [ 0, 1 ]

- name: spmm_32_8_128
  category: nightly
  function:
    spmm: *real_precisions_2b
  M: [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]
  N: 32
  K: 32
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_48_8_128
  category: nightly
  function:
    spmm: *real_precisions_2b
  M: 48
  N: [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]
  K: 48
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_64_8_128
  category: nightly
  function:
    spmm: *real_precisions_2b
  M: 64
  N: 64
  K: [16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_64_8
  category: quick
  function:
    spmm: *real_precisions_2b
  M: 64
  N: 8
  K: 8
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_8_64
  category: quick
  function:
    spmm: *real_precisions_2b
  M: 8
  N: 8
  K: 64
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range


- name: spmm_96
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 96
  N: 96
  K: 96
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_128
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 128
  N: 128
  K: 128
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_256
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 256
  N: 256
  K: 256
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_256_8_16
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 256
  N: 8
  K: 16
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_16_256_8
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 16
  N: 256
  K: 8
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_8_16_256
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 8
  N: 16
  K: 256
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_512
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 512
  N: 512
  K: 512
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_1024
  category: nightly
  function:
    spmm: *real_precisions_2b
  M: 1024
  N: 1024
  K: 1024
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: spmm_bf16
  category: quick
  function:
    spmm: *hpa_bf16_precision
  transA: T
  transB: N
  alpha: 1
  beta: 0
  matrix_size:
    - { M:  512, N:  512, K:  512 }
    - { M: 1024, N: 1024, K: 1024 }
    - { M: 2048, N: 2048, K: 2048 }
    - { M: 4096, N: 4096, K: 4096 }
    - { M:  960, N: 1024, K: 1024 }
    - { M: 3840, N: 4096, K: 4096 }

- name: spmm_small2
  category: pre_checkin
  transA: N
  transB: N
  function:
    spmm: *real_precisions_2b
  matrix_size:
    - { M:  512, N:  512, K:  512 }
    - { M: 960,  N: 1024, K: 1024 }
    - { M: 1024, N: 1024, K: 1024 }
  alpha: [ 0.0, 0.5, 1.0 ]
  beta:  [ 0.0, 0.5, 1.0 ]

- name: spmm_128_activation
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 128
  N: 128
  K: 128
  transA: N
  transB: N
  alpha: 1
  beta: 0
  activation_type: [ none, relu, abs]

- name: spmm_128_activation_sigmoid
  category: pre_checkin
  function:
    spmm: *activation_sigmoid_tanh_precisions
  M: 128
  N: 128
  K: 128
  transA: N
  transB: N
  alpha: 1
  beta: 0
  activation_type: [sigmoid]

- name: spmm_128_activation_tanh
  category: pre_checkin
  function:
    spmm: *activation_sigmoid_tanh_precisions
  M: 128
  N: 128
  K: 128
  transA: N
  transB: N
  alpha: 1
  beta: 0
  activation_type: [tanh]
  activation_arg1 : [-1.0, 0.0, 0.5, 1.0]
  activation_arg2 : [-1.0, 0.0, 0.5, 1.0, 3.0]

- name: spmm_128_activation_leakyrelu
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 128
  N: 128
  K: 128
  transA: N
  transB: N
  alpha: 1
  beta: 0
  activation_type: [leakyrelu, gelu]
  activation_arg1 : [-1.0, 0.0, 0.5, 1.0]

- name: spmm_128_activation_clippedrelu
  category: pre_checkin
  function:
    spmm: *real_precisions_2b
  M: 128
  N: 128
  K: 128
  transA: N
  transB: N
  alpha: 1
  beta: 0
  activation_type: clippedrelu
  activation_arg1 : [-1.0, 0.0, 0.5]
  activation_arg2 : [-1.0, 0.0, 0.5, 1.0, 3.0]

- name: aux_plan_assign
  category: pre_checkin
  function:
    aux_plan_assign: *hpa_int8_precision
  M: 128
  N: 128
  K: 128
  transA: T
  transB: N
  alpha: 1
  beta: 0

...

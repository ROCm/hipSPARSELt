---
include: hipsparselt_common.yaml
include: known_bugs.yaml
include: spmm_common_16.yaml

Definitions:
  - &alpha_beta_range
    - { alpha:  5, beta:  0 }
    - { alpha:  0, beta:  3 }
    - { alpha:  1, beta:  3 }
    - { alpha:  1, beta:  1 }

  - &alpha_beta_range_small
    - { alpha: 2, alphai: 2, beta: -1.0, betai: 2.0 }

  - &transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }
    - { transA: T, transB: T }

  - &deepbench_alpha_beta_range
    - { alpha: 1, beta: 0 }
    - { alpha: 1, beta: 1 }

  - &deepbench_transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }

  - &ldd_transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }
    - { transA: T, transB: T }


Tests:
- name: spmm_bad_arg
  category: pre_checkin
  function:
    - spmm_bad_arg: *real_precisions_1b
  transA: N
  transB: N
  fortran: [ false, true ]

# Tests confirm no NaN propagation when alpha = 0, 2 and beta = 0. Value .NaN is converted into zero
- {name: alpha_beta_zero_NaN, category: pre_checkin, precision: *real_precisions_1b,
   function: spmm, transA: N, transB: N, M: 256, N: 128, K:  64, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }


# Split *real_precisions_1b into *int8 and *nonint8_real_precisions. Since int8 has flags 0,1

- name: spmm_small
  category: quick
  function:
    spmm: *real_precisions_1b
  matrix_size: *small_matrix_size_range
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range
  sparse_b: [true, false]
  alpha_vector_scaling: [true, false]

- name: spmm_small
  category: quick
  function:
    spmm: *real_precisions_1b
  matrix_size: *small_matrix_size_range
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range
  bias_vector: [true]
  bias_stride: [0, -1, 256]
  sparse_b: [true, false]
  alpha_vector_scaling: [true, false]

- name: spmm_medium
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  matrix_size: *medium_matrix_size_range
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range
  sparse_b: [true, false]

- name: spmm_medium_HMM
  category: HMM
  function:
    spmm: *real_precisions_1b
  matrix_size: *medium_matrix_size_range
  transA: [ N ]
  transB: [ N ]
  alpha: 1
  beta: 1
  HMM: true
  sparse_b: [true, false]

- name: spmm_chunk
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  matrix_size: *chunk_matrix_size_range
  transA_transB: *transA_transB_range
  alpha: 2
  beta: 3
  sparse_b: [true, false]

# Split *int8_half_single_precisions into *int8 and *half_single_precisions. Since int8 has flags 0,1

- name: spmm_deepbench
  category: nightly
  function:
    spmm: *real_precisions_1b
  matrix_size: *deepbench_sizes
  alpha_beta: *deepbench_alpha_beta_range
  transA_transB: *deepbench_transA_transB_range
  sparse_b: [true, false]

- name: resnet50_fwd
  category: nightly
  function:
    spmm: *real_precisions_1b
  transA: N
  transB: N
  matrix_size: *resnet50_fwd_sizes
  alpha: 1
  beta: 0
  sparse_b: [true, false]
  orderA: [C, R]
  orderB: [C, R]

- name: resnet50_bwdwrw
  category: nightly
  function:
    spmm: *real_precisions_1b
  transA: T
  transB: N
  matrix_size: *resnet50_bwdwrw_sizes
  alpha: 1
  beta: 1
  sparse_b: [true, false]

- name: resnet50_bwddata
  category: nightly
  function:
    spmm: *real_precisions_1b
  transA: N
  transB: T
  matrix_size: *resnet50_bwddata_sizes
  alpha: 1
  beta: 0
  sparse_b: [true, false]

- name: inception4_fwd
  category: nightly
  function:
    spmm: *real_precisions_1b
  transA: N
  transB: N
  matrix_size: *inception4_fwd_sizes
  alpha: 1
  beta: 0
  sparse_b: [true, false]

- name: inception4_bwdwrw
  category: nightly
  function:
    spmm: *real_precisions_1b
  transA: T
  transB: N
  matrix_size: *inception4_bwdwrw_sizes
  alpha: 1
  beta: 1
  sparse_b: [true, false]

- name: inception4_bwddata
  category: nightly
  function:
    spmm: *real_precisions_1b
  transA: N
  transB: T
  matrix_size: *inception4_bwddata_sizes
  alpha: 1
  beta: 0
  sparse_b: [true, false]

- name: ctest_bwdwrw
  category: nightly
  function:
    spmm: *real_precisions_1b
  transA: T
  transB: N
  alpha: 1
  beta: 1
  matrix_size: *ctest_bwdwrw_sizes
  sparse_b: [true, false]

- name: ctest_fwd
  category: nightly
  function:
    spmm: *real_precisions_1b
  transA: N
  transB: N
  alpha: 1
  beta: 0
  matrix_size: *ctest_fwd_sizes
  sparse_b: [true, false]

- name: spmm_16
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 16
  K: 16
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_algo
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 32
  N: 32
  K: 32
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  spmm_algo: [ 0, 1 ]
  sparse_b: [true, false]

- name: spmm_32_16_128
  category: nightly
  function:
    spmm: *real_precisions_1b
  M: [16, 32, 48, 64, 80, 96, 112, 128]
  N: 32
  K: 32
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_48_16_128
  category: nightly
  function:
    spmm: *real_precisions_1b
  M: 48
  N: [16, 32, 48, 64, 80, 96, 112, 128]
  K: 48
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_64_16_128
  category: nightly
  function:
    spmm: *real_precisions_1b
  M: 64
  N: 64
  K: [16, 32, 48, 64, 80, 96, 112, 128]
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_64_16_16
  category: quick
  function:
    spmm: *real_precisions_1b
  M: 64
  N: 16
  K: 16
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_16_64
  category: quick
  function:
    spmm: *real_precisions_1b
  M: 16
  N: 16
  K: 64
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_96
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 96
  N: 96
  K: 96
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_128
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 128
  N: 128
  K: 128
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_256
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 256
  N: 256
  K: 256
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_256_16_16
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 256
  N: 16
  K: 16
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_16_256_16
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 16
  N: 256
  K: 16
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_9_12_13_16_256
  category: pre_checkin
  function:
    spmm: *real_precisions_1b

  M: 16
  N: 16
  K: 256
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_512
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 512
  N: 512
  K: 512
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_1024
  category: nightly
  function:
    spmm: *real_precisions_1b
  M: 1024
  N: 1024
  K: 1024
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  sparse_b: [true, false]

- name: spmm_bf16
  category: quick
  function:
    spmm: *hpa_bf16_precision
  transA: T
  transB: N
  alpha: 1
  beta: 0
  matrix_size:
    - { M:  512, N:  512, K:  512 }
    - { M: 1024, N: 1024, K: 1024 }
    - { M: 2048, N: 2048, K: 2048 }
    - { M: 4096, N: 4096, K: 4096 }
    - { M:  960, N: 1024, K: 1024 }
    - { M: 3840, N: 4096, K: 4096 }
  sparse_b: [true, false]

- name: spmm_small2
  category: pre_checkin
  transA: N
  transB: N
  function:
    spmm: *real_precisions_1b
  matrix_size:
    - { M:  512, N:  512, K:  512 }
    - { M: 960,  N: 1024, K: 1024 }
    - { M: 1024, N: 1024, K: 1024 }
  alpha: [ 0.0, 0.5, 1.0 ]
  beta:  [ 0.0, 0.5, 1.0 ]
  sparse_b: [true, false]

- name: spmm_128_activation
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 128
  N: 128
  K: 128
  transA: N
  transB: N
  alpha: 1
  beta: 0
  sparse_b: [true, false]

- name: spmm_128_activation_leakyrelu
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 128
  N: 128
  K: 128
  transA: N
  transB: N
  alpha: 1
  beta: 0
  activation_type: [leakyrelu, gelu]
  activation_arg1 : [-1.0, 0.0, 0.5, 1.0]
  sparse_b: [true, false]

- name: spmm_128_activation_clippedrelu
  category: pre_checkin
  function:
    spmm: *real_precisions_1b
  M: 128
  N: 128
  K: 128
  transA: N
  transB: N
  alpha: 1
  beta: 0
  activation_type: clippedrelu
  activation_arg1 : [-1.0, 0.0, 0.5]
  activation_arg2 : [-1.0, 0.0, 0.5, 1.0, 3.0]
  sparse_b: [true, false]
...
